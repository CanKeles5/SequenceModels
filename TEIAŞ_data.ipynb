{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEIAŞ-data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH/qT7rLJ+x/uIovKEWnZz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CanKeles5/SequenceModels/blob/main/TEIA%C5%9E_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pe94wvjeVIK"
      },
      "source": [
        "import torch\r\n",
        "torch.cuda.is_available()\r\n",
        "if torch.cuda.is_available():  \r\n",
        "  dev = \"cuda:0\" \r\n",
        "else:  \r\n",
        "  dev = \"cpu\"  \r\n",
        "device = torch.device(dev)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLnNKJCleZIW",
        "outputId": "3f585e4c-c7a9-4b24-80ef-e9a0fd9c8278"
      },
      "source": [
        "device"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQcVLbbQncsR"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y101PUHcuAh",
        "outputId": "37e9e5f2-c378-4e23-9c39-b2ebe0d59440"
      },
      "source": [
        "! pip install pickle5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp36-cp36m-linux_x86_64.whl size=218620 sha256=7d10e2d1edc26863e0fc8e8f9d9f913dc466a9a28200cb19cadcdec911e73000\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgRPeJqndlf"
      },
      "source": [
        "import pickle5\r\n",
        "from math import sqrt\r\n",
        "from datetime import datetime\r\n",
        "from numpy import concatenate\r\n",
        "from matplotlib import pyplot\r\n",
        "from pandas import read_csv\r\n",
        "from pandas import DataFrame\r\n",
        "from pandas import concat\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iWe2exWcKGo"
      },
      "source": [
        "with open('/content/20210101-epias-raw.pkl', 'rb') as f:\r\n",
        "    data = pickle5.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpC0YYEdjJi-"
      },
      "source": [
        "Drop all colums of zeros: df.loc[:, (df != 0).any(axis=0)]\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORkCVX7vil21"
      },
      "source": [
        "new_data = data.loc[:, (data != 0).any(axis=0)] #(index=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsnV-rVp4PC7"
      },
      "source": [
        "new_data = new_data.drop(['date', 'ltotal',\t'lwind',\t'lbiogas',\t'lcanalType',\t'lbiomass',\t'lsun',\t'lothers'], axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3jtf70TnWcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "5ef6d99a-90e5-4c14-966b-245a4f2d1cbf"
      },
      "source": [
        "(new_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fueloil</th>\n",
              "      <th>blackCoal</th>\n",
              "      <th>lignite</th>\n",
              "      <th>geothermal</th>\n",
              "      <th>naturalGas</th>\n",
              "      <th>river</th>\n",
              "      <th>dammedHydro</th>\n",
              "      <th>lng</th>\n",
              "      <th>biomass</th>\n",
              "      <th>naphta</th>\n",
              "      <th>importCoal</th>\n",
              "      <th>asphaltiteCoal</th>\n",
              "      <th>wind</th>\n",
              "      <th>sun</th>\n",
              "      <th>importExport</th>\n",
              "      <th>total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29.2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>551.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2326.01</td>\n",
              "      <td>151.49</td>\n",
              "      <td>502.38</td>\n",
              "      <td>9.3</td>\n",
              "      <td>26.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1184.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4781.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29.1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>544.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2334.78</td>\n",
              "      <td>152.77</td>\n",
              "      <td>441.94</td>\n",
              "      <td>9.3</td>\n",
              "      <td>27.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1184.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4724.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>547.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2157.76</td>\n",
              "      <td>151.47</td>\n",
              "      <td>430.19</td>\n",
              "      <td>9.3</td>\n",
              "      <td>26.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1185.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4538.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>547.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2084.72</td>\n",
              "      <td>125.83</td>\n",
              "      <td>275.18</td>\n",
              "      <td>9.4</td>\n",
              "      <td>25.67</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1183.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4280.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28.3</td>\n",
              "      <td>0.00</td>\n",
              "      <td>549.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2034.68</td>\n",
              "      <td>124.35</td>\n",
              "      <td>271.86</td>\n",
              "      <td>9.4</td>\n",
              "      <td>26.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1066.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4111.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67263</th>\n",
              "      <td>36.3</td>\n",
              "      <td>625.95</td>\n",
              "      <td>4916.02</td>\n",
              "      <td>1237.17</td>\n",
              "      <td>8912.15</td>\n",
              "      <td>1252.61</td>\n",
              "      <td>6215.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>581.21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7909.62</td>\n",
              "      <td>220.80</td>\n",
              "      <td>1265.85</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-167.33</td>\n",
              "      <td>33005.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67264</th>\n",
              "      <td>36.2</td>\n",
              "      <td>620.18</td>\n",
              "      <td>4808.25</td>\n",
              "      <td>1238.56</td>\n",
              "      <td>9068.42</td>\n",
              "      <td>1153.29</td>\n",
              "      <td>5573.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>563.34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7795.01</td>\n",
              "      <td>223.00</td>\n",
              "      <td>1262.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-151.42</td>\n",
              "      <td>32190.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67265</th>\n",
              "      <td>36.9</td>\n",
              "      <td>637.67</td>\n",
              "      <td>4755.74</td>\n",
              "      <td>1244.27</td>\n",
              "      <td>9164.25</td>\n",
              "      <td>1058.40</td>\n",
              "      <td>4897.53</td>\n",
              "      <td>0.0</td>\n",
              "      <td>561.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7772.89</td>\n",
              "      <td>223.00</td>\n",
              "      <td>1286.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-159.21</td>\n",
              "      <td>31478.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67266</th>\n",
              "      <td>36.6</td>\n",
              "      <td>651.68</td>\n",
              "      <td>4765.78</td>\n",
              "      <td>1243.95</td>\n",
              "      <td>9240.85</td>\n",
              "      <td>1026.27</td>\n",
              "      <td>4426.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>567.91</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7785.29</td>\n",
              "      <td>220.80</td>\n",
              "      <td>1219.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-169.87</td>\n",
              "      <td>31014.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67267</th>\n",
              "      <td>37.2</td>\n",
              "      <td>653.61</td>\n",
              "      <td>4852.70</td>\n",
              "      <td>1231.10</td>\n",
              "      <td>9189.95</td>\n",
              "      <td>964.30</td>\n",
              "      <td>3631.73</td>\n",
              "      <td>0.0</td>\n",
              "      <td>576.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7778.35</td>\n",
              "      <td>220.79</td>\n",
              "      <td>1201.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>30338.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67268 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       fueloil  blackCoal  lignite  ...  sun  importExport     total\n",
              "0         29.2       0.00   551.00  ...  0.0          0.00   4781.73\n",
              "1         29.1       0.00   544.00  ...  0.0          0.00   4724.96\n",
              "2         29.1       0.00   547.00  ...  0.0          0.00   4538.70\n",
              "3         29.0       0.00   547.00  ...  0.0          0.00   4280.80\n",
              "4         28.3       0.00   549.00  ...  0.0          0.00   4111.04\n",
              "...        ...        ...      ...  ...  ...           ...       ...\n",
              "67263     36.3     625.95  4916.02  ...  0.0       -167.33  33005.46\n",
              "67264     36.2     620.18  4808.25  ...  0.0       -151.42  32190.32\n",
              "67265     36.9     637.67  4755.74  ...  0.0       -159.21  31478.88\n",
              "67266     36.6     651.68  4765.78  ...  0.0       -169.87  31014.77\n",
              "67267     37.2     653.61  4852.70  ...  0.0          0.00  30338.20\n",
              "\n",
              "[67268 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9OAdsuSxPSl"
      },
      "source": [
        "new_data = new_data.to_numpy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_4GBZbldZX-"
      },
      "source": [
        "To-do:\r\n",
        "  Get the data in such a format that it can work with the other notebook.\r\n",
        "  \r\n",
        "  1- Extract the date element.\r\n",
        "\r\n",
        "  2- Normalize all features in the input part of the data.\r\n",
        "\r\n",
        "  3- Create the inputs and the targets.\r\n",
        "\r\n",
        "  4- Create train and test sets.\r\n",
        "\r\n",
        "  5- \r\n",
        "\r\n",
        "  6- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szubISeMboQM"
      },
      "source": [
        "# convert series to supervised learning\r\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\r\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\r\n",
        "\tdf = DataFrame(data)\r\n",
        "\tcols, names = list(), list()\r\n",
        "\t# input sequence (t-n, ... t-1)\r\n",
        "\tfor i in range(n_in, 0, -1):\r\n",
        "\t\tcols.append(df.shift(i))\r\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\r\n",
        "\t# forecast sequence (t, t+1, ... t+n)\r\n",
        "\tfor i in range(0, n_out):\r\n",
        "\t\tcols.append(df.shift(-i))\r\n",
        "\t\tif i == 0:\r\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\r\n",
        "\t\telse:\r\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\r\n",
        "\t# put it all together\r\n",
        "\tagg = concat(cols, axis=1)\r\n",
        "\tagg.columns = names\r\n",
        "\t# drop rows with NaN values\r\n",
        "\tif dropnan:\r\n",
        "\t\tagg.dropna(inplace=True)\r\n",
        "\treturn agg\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz_o3Bf4boM1",
        "outputId": "b7189193-ac1e-4849-f24b-86e31f287ade"
      },
      "source": [
        "# integer encode direction\r\n",
        "#encoder = LabelEncoder()\r\n",
        "#values[:,4] = encoder.fit_transform(values[:,4])\r\n",
        "# ensure all data is float\r\n",
        "\r\n",
        "#values = values.astype('float32')\r\n",
        "# normalize features\r\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\r\n",
        "scaled = scaler.fit_transform(new_data)\r\n",
        "# specify the number of lag hours\r\n",
        "n_hours = 1\r\n",
        "n_features = 15\r\n",
        "# frame as supervised learning\r\n",
        "\r\n",
        "reframed = series_to_supervised(scaled, n_hours, 1)\r\n",
        "print(reframed.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(67267, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmDHPWk343jJ"
      },
      "source": [
        "reframed = reframed.to_numpy()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR_bXll1JO-X",
        "outputId": "af0d2e15-aa1f-424b-8592-d28dd91798c1"
      },
      "source": [
        "n_train_hours = 50000\r\n",
        "train = reframed[:n_train_hours, :]\r\n",
        "test = reframed[n_train_hours:, :]\r\n",
        "# split into input and outputs\r\n",
        "n_obs = n_hours * n_features\r\n",
        "train_X, train_y = train[:, :n_obs], train[:, -n_features]\r\n",
        "test_X, test_y = test[:, :n_obs], test[:, -n_features]\r\n",
        "print(train_X.shape, len(train_X), train_y.shape)\r\n",
        "# reshape input to be 3D [samples, timesteps, features]\r\n",
        "train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))\r\n",
        "test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))\r\n",
        "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\r\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 15) 50000 (50000,)\n",
            "(50000, 1, 15) (50000,) (17267, 1, 15) (17267,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-5gYhr6LGgT",
        "outputId": "3116f7f4-2e72-4d88-ba48-cd0c41ffcb8e"
      },
      "source": [
        "train_X.shape[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z51TQBohLIJ3",
        "outputId": "6abd4117-4a54-4909-ea74-c83840904ece"
      },
      "source": [
        "test_X.shape[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17267"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48mDYPiiJfxt"
      },
      "source": [
        "class LSTM(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super(LSTM, self).__init__()\r\n",
        "\r\n",
        "    self.lstm = nn.LSTM(input_size=15, hidden_size=50, num_layers=1)\r\n",
        "    self.FC = nn.Linear(50, 1)\r\n",
        "\r\n",
        "  def forward(self, input):\r\n",
        "    pred, _ = self.lstm(input)\r\n",
        "\r\n",
        "    pred = self.FC(pred.view(50))\r\n",
        "\r\n",
        "    return pred"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF0ge_gOJiTi"
      },
      "source": [
        "model = LSTM().to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iuxhjBXJiQm"
      },
      "source": [
        "loss_function = nn.L1Loss()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-6)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwdaiI0bJiNY"
      },
      "source": [
        "train_loss_hist = []\r\n",
        "test_loss_hist = []"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "70IW6m21JiKJ",
        "outputId": "fe1a0e2f-4891-4a2d-92eb-40f2d4611d7c"
      },
      "source": [
        "for epoch in range(20):\r\n",
        "  train_loss = 0.0\r\n",
        "  test_loss = 0.0\r\n",
        "\r\n",
        "  for i in range(train_X.shape[0]):\r\n",
        "    data = torch.Tensor(train_X[i]).unsqueeze(0)\r\n",
        "    data = data.to(device)\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    prediction = model(data)\r\n",
        "\r\n",
        "    #print(\"Train --- prediction: \" + str(prediction) + \" , GT: \" + str(train_y[i]))\r\n",
        "\r\n",
        "    loss = loss_function(prediction, torch.Tensor([train_y[i]]))\r\n",
        "\r\n",
        "    train_loss += loss.item()\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for i in range(test_X.shape[0]):\r\n",
        "      data = torch.Tensor(test_X[i]).unsqueeze(0)\r\n",
        "      data = data.to(device)\r\n",
        "\r\n",
        "      prediction = model(data)\r\n",
        "\r\n",
        "      #print(\"Validate --- prediction: \" + str(prediction) + \" , GT: \" + str(test_y[i]))\r\n",
        "\r\n",
        "      loss = loss_function(prediction, torch.Tensor([test_y[i]]))\r\n",
        "      test_loss += loss.item()\r\n",
        "\r\n",
        "  train_loss = train_loss / train_X.shape[0]\r\n",
        "  test_loss = test_loss / test_X.shape[0]\r\n",
        "\r\n",
        "  train_loss_hist.append(train_loss)\r\n",
        "  test_loss_hist.append(test_loss)\r\n",
        "\r\n",
        "  print(\"Train loss: \" +  str(train_loss) + \" --- Test loss: \" + str(test_loss))\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.3552082933442295 --- Test loss: 0.1875191728038145\n",
            "Train loss: 0.09836252043500543 --- Test loss: 0.14585006359487593\n",
            "Train loss: 0.09213301954463124 --- Test loss: 0.14343007538786304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-10e4916508ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "V0VMCc4zJiHN",
        "outputId": "e64af35c-fb02-40de-8603-369f0ca30662"
      },
      "source": [
        "pyplot.plot(train_loss_hist)\r\n",
        "pyplot.plot(test_loss_hist)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7faeb00341d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KFvJE4IJjhK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMNzzBZ6JjeO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2TG7y-XJjbF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}